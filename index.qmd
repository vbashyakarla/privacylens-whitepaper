```{css, include = FALSE}
<!-- .quarto-title-block { -->
<!-- background-color: #123445; -->
<!-- padding: 20px; -->
<!-- border-radius: 5px; -->
<!-- } -->

/* Move TOC to the left */
body {
  display: flex;
}

#quarto-toc {
  order: -1;              /* Moves TOC to the left */
  width: 250px;           /* Set desired width */
  position: fixed;        /* Keep TOC fixed on the left */
  top: 0;                 /* Align with the top of the page */
  left: 0;                /* Position it at the left edge */
  bottom: 0;              /* Extend to the bottom */
  overflow-y: auto;       /* Enable vertical scroll if needed */
}

.content {
  margin-left: 260px;     /* Offset content to the right so it's not hidden behind TOC */
  width: auto;
}
```

       
![](pl_logo.png){width=40%} 


```{r, include=FALSE}
library(gridExtra)
library(here)
library(magick)
library(patchwork)
library(tidyverse)

theme_set(theme_linedraw())

#| fig-width: 12  # Adjust width (default is ~7)
#| fig-height: 5  # Adjust height if needed
#| fig.align: "center"
```

\newpage

## Executive Summary

There are two primary research questions at hand. First, can the privacy scores output by PrivacyLens be trusted? Second, if there is reason to not dismiss the scores generated by PrivacyLens, what can be learned about the privacy of domains used to power Google Gemini's AI summaries relative to domains served in organic search results?

This following analysis finds that there is some consistency ($\rho = 0.33$) between raw scores output by PrivacyLens via ChatGPT and those generated by Gemini when instructed to score the some domains using the same rubric. While these results suggest some grounds for reliability of the scores output by PrivacyLens, a proper validation study should be conducted with gold standard scores generated by experts.

Next, it appears as if Google Gemini uses domains with roughly consistent privacy scores on the whole when generating its AI summaries compared to the domains that appear in its search organic search results (pages 1 through 5). There may be evidence that relative to the first five pages of organic Google search results, domains used in Gemini summaries tend to have better privacy scores on the whole, though this trend is largely influenced by slightly higher-risk domains in the results on pages 3, 4, and 5. Relative to search results on pages 1 and 2, Gemini domains may be slightly weaker privacy-wise, but these observations cannot be made conclusively and would need more information to test robustly. When considering the proportion of specifically high-risk domains, the proprotion of high-risk domains shown to the user appears to increase when navigating further away from Google's Gemini summaries. These observations should be validated through a more comprehensive, independent study that addresses some of the methodological challenges outlined below.

## Motivation and Background

Google’s May 2024 blog post, “Generative AI in Search: Let Google do the searching for you” features a promotional video explaining the ways in which Gemini summaries will enhance work. The accompanying promotional video claims it will “simplify” search. The company reports that their experiments demonstrate that “with AI Overviews, people use Search more, and are more satisfied with their results.” The company also states that, “With AI Overviews, people are visiting a greater diversity of websites for help with more complex questions. And we see that the links included in AI Overviews get more clicks than if the page had appeared as a traditional web listing for that query.” [@GoogleGenAI] An estimated 1.5% of domains appear to land in Google’s AI overviews, suggesting a high degree of selectivity [@Baek2025]. In other words, relatively few domains appear in Google AI summaries, and linking to sources in Gemini’s overviews is associated with greater traffic to those links, suggesting that perhaps people are indeed visiting links embedded in Gemini’s summaries.

Against this backdrop, it is natural to ask **how the privacy of websites used to power Gemini’s textual summaries compares to the privacy of websites appearing in Google’s search results**. The analysis here addresses this question in two parts. First, it assesses the reliability of privacy scores generated by [PrivacyLens](https://www.privacylens.info/). Next, it evaluates how the privacy scores of websites in Google's Gemini summaries compares to the privacy scores of other Google search results.

## Methodology

Before comparing the privacy scores of websites included in Gemini's summaries and those in Google's (sponsored and organic) search results, it is first necessary to assess the quality of the privacy score being used. Are these scores reliable, and what evidence justifies their use?

### Consistency Check

Ideally, a robust validation study would have been conducted in which expert academics and grad students would manually score a handful of privacy policies subject to the same rubric given to PrivacyLens. These scores would be considered our sources of truth, and we could then compare PrivacyLens's output scores against these gold standards.

Without the budget to conduct a proper study to hire experts, we decided to compare the outputs from PrivacyLens (the scores of which are output by ChatGPT) to those of another LLM, Google Gemini and measure the extent to which the scores demonstrated consistency with one another. Though this approach does not tell us whether the scores themselves are accurate, it could be a sign for the extent to which the scores are precise. Still, it is of course still possible that both Chat GPT and Gemini suffer from the similar imprecisions. Despite these methodological challenges, a collection of about 100 domains in recent Google search results were selected. Their privacy scores (raw scores from 0 to 68) were look up in PrivacyLens's database and collected on Google Gemini using a variety of different Gemini models: 2.0 Flash, 2.0 Flash Thinking, 2.5 Pro, and Deep Research.

### Gemini Summary Evaluation

To investigate patterns in the privacy scores from Google search results, domains across a variety of health-related queries were collected. In each case, the domains from the first five pages of results were stored, and each domain was associated with a source designating where in Google search the domain originated: `Gemini Summary`, `Page 1 Organic`, `Page 2 Organic`, `Page 3 Organic`, `Page 4 Organic`, `Page 5 Organic`, and `Sponsored Result`. The same domain could be associated with multiple sources. For example, if the same domain were included in a Gemini Summary and shown on Page 1 of the organic search results, the domain would be noted twice in the collected data. However, if the same domain appeared multiple times within the same section of the search results, as with `Reddit` in the example below, then the domain is counted only once. Future studies may wish to count domains in proportion to their relative appearance in search results.

![If a domain appears repeatedly within the same section of the search results, it is counted only once.](repeated_domains.png)

```{=html}
<!---
PAPER_PUBLICATION
Explain why we chose to focus on unique domains
(1) domains are sometimes grouped together (see quora / reddit image)
(2) people don't click randomly
(3) a single search result may be associated with 4 links, while another single search reuslt may be associated with fewer results. Such a weighting doesn't appear to correctly represent how people interact with Google
(4) people don't click on results randomly
-->
```

The health-related queries used for this data collection exercise were:

-   What does a rash on my arm mean?
-   How to cope with anxiety?
-   How to increase chances of getting pregnant?
-   Tips to live with fibromyalgia
-   What is intermittent fasting and how does it work?
-   What are the benefits of acupuncture?
-   What are the steps of a colonoscopy?
-   How to appeal a denied insurance claim?
-   How to rescue someone who is choking?
-   What are the best at-home blood pressure monitors to purchase?

The results were collected from a signed-in Google account in Berkeley, California, in which search personalization and web app activity was [turned off](https://support.google.com/websearch/answer/12410098?hl=en), as shown below.

![Google search reuslts were compiled from a Google account geolocated to Berkeley without personalization.](no_personalization.png)

For the sake of reprodicibility, domains were extracted from the search results using an auto-generated script whose results were manually examined for accuracy. While the script occassionally misclassified `Sponsored` and `Organic` search results, it performed fairly on pages 2 through 5 of each query. However, it failed to cleanly distinguish between `gemini_summary` and `organic_result_page_1` results. For this reason, these data were collected manually. All 50 `.html` files from the Google search results were archived using the Chrome plugin `SingleFile` and are available in [here](https://github.com/vbashyakarla/privacylens-whitepaper/tree/main/query_results). The [script](https://github.com/vbashyakarla/privacylens-whitepaper/blob/main/extract-domains.py) used to extract domains is also available in our repository.

## Results

### LLM Consistency Results: Chat GPT vs. Gemini

The sample of domains used in this analysis deviates strongly from random. As @fig-dist shows, the distribution of domain scores is even (i.e,. $\frac{1}{3}$ assigned to each category weak, moderate, and strong). Based on the data shown in the plot of scores by model alone (right), it appears as if Chat GPT scores websites more leniently. The proportion of domains scored `weak` by Gemini (about 38%) is distinctly in the scores output by Gemini relative to those of Chat GPT (about 24%).

```{r}
cons_df <- read_csv("consistency_data.csv")
```

```{r}
scored_df <-
cons_df |>
gather("chat_gpt_score", "gemini_score", key = "LLM", value = "score") |>
mutate(score_category = case_when(
score > 56 ~ "strong",
score >= 40 & score <= 56 ~ "moderate",
score < 40 ~ "weak",
TRUE ~ "NA"  # The default case
)) |>
mutate(score_category = factor(score_category,
levels = c("weak", "moderate", "strong"),
ordered = TRUE))

scored_llms_df <-
scored_df |>
pivot_wider(names_from = LLM, values_from = c(score, score_category)) |>
mutate(match_binary = case_when(
score_category_chat_gpt_score ==  score_category_gemini_score ~ "match",
TRUE ~ "no match"  # The default case
)) |>
mutate(match_detail = case_when(
score_category_chat_gpt_score ==  score_category_gemini_score ~ "match",
score_category_chat_gpt_score > score_category_gemini_score ~
"chat_gpt_higher",
TRUE ~ "gemini_higher"  # The default case
))
```

```{r, include = FALSE}
score_dist_plot <- scored_df |>
ggplot(aes(x = score_category, fill = score_category)) +
geom_bar(stat = "count") +
scale_fill_brewer(palette = "Set1") +
theme_light() +
xlab("Score Category") +
ylab("Count") +
ggtitle("Distribution of Scores\n by Category") +
guides(fill = "none")

score_dist_plot
```

```{r, include = FALSE}
scores_by_model <- score_dist_plot +
facet_wrap(LLM ~ ., ncol = 1) +
ggtitle("Distribution of Scores\n by Category by LLM")

scores_by_model
```

```{r, include = FALSE}
grid.arrange(score_dist_plot, scores_by_model, ncol = 2)
```

```{r}
#| label: fig-dist
#| fig-cap: "Privacy score distributions overall (left) and by model (right)"
#| fig.width: 7
#| fig.height: 4

combined_dist_plots <- score_dist_plot +
scores_by_model +
plot_layout(ncol = 2) +
theme_linedraw()

combined_dist_plots
```

Among scores output by Google Gemini itself, the distribution of score buckets is not even across all model versions, as @fig-gemini-version shows. For instance, Gemini 2.5 Pro appears most lenient, about 80% of domains scored by this version were classified as `strong`. However, it is possible that the privacy scores of the underlying websites did legitimately differ. To study this phenonenon more closely in the future, the same domain should be scored by different Gemini versions, much in the same way Chat GPT and Gemini are being compared against one another here.

```{r, include = FALSE}
scores_by_gemini_version <- score_dist_plot +
facet_wrap(gemini_version ~ ., ncol = 1, scales = "free_y") +
ggtitle("Distribution of Scores by Gemini Model")

scores_by_gemini_version
```

```{r, include = FALSE}
within_model_score_plot <- scored_df |>
group_by(gemini_version, score_category) |>
tally() |>
left_join(
scored_df |>
group_by(gemini_version) |>
tally() |>
rename(total_n = n)) |>
mutate(pct = n / total_n) |>
ggplot(aes(x = score_category, y = pct, fill = gemini_version)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.75) +
scale_fill_brewer(palette = "RdYlBu",
name = "Gemini Model",
labels = c("2.0 Flash",
"2.0 Flash Thinking",
"2.5 Pro",
"Deep Research")) +
xlab("Score Category") +
ylab("Proportion of Within-Model Scores") +
ggtitle("Distribution of Gemini Scores by Model") +
scale_y_continuous(labels = scales::percent_format())
```

```{r}
#| label: fig-gemini-version
#| fig-cap: "The plot above shows how each Google Gemini model vesion used here evaluted different domains. Surprisngly, the distribution is not even. Based on these results alone, Gemini 2.5 Pro appears to be the most lenient, while 2.0 Flash Thinking appears the strictest. Still, further study is needed, as the observed results may be attributable to the domains at hand since the same domains were not scored by multiple Gemini model versions."
#| fig.width: 7
#| fig.height: 5

within_model_score_plot
```

@fig-corr illustrates that the correlation of privacy scores output by Chat GPT and those output by Gemini is 0.33. If the scores had aligned completely (i.e, $\rho$ of 1), then all the points would have fallen along the gray dotted line. The linear regression line is shown in blue. The points in green correspond to domains in which both models output the same score category, while those in orange diverge. A closer analysis of which sites are over-(under-)evaluated by which model is a natural follow-up to this observation.

```{r, include = FALSE}
cor_vals <- scored_llms_df |>
summarise(correlation = cor(score_chat_gpt_score, score_gemini_score),
corr_spearman = cor(as.numeric(score_category_chat_gpt_score),
as.numeric(score_category_gemini_score),
method = "spearman"),
corr_kendall = cor(score_chat_gpt_score, score_gemini_score,
method = "kendall"))

cor_value <- cor_vals |>  select(correlation) |>  pull()
```

```{r, include = FALSE}
llm_corr_plot <- scored_llms_df |>
ggplot(aes(x = score_chat_gpt_score, y = score_gemini_score,
color = match_binary)) +
geom_point(size = 1.8) +
scale_color_brewer(palette = "Dark2",
labels = c("Match", "Mismatch")) +
geom_smooth(method = "lm", se = FALSE, color = "navy") +
xlab("Chat GPT Score") +
ylab("Gemini Score") +
ggtitle("Gemini vs. Chat GPT Raw Scores") +
labs(color = "Comparision \nOutcome") +
annotate("text",
x = 12, y = 61,
label = bquote(rho~"="~.(round(cor_value, 2))),
size = 5, color = "black") +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray")

llm_corr_plot
```

```{r}
#| label: fig-corr
#| fig-cap: "The scores output by both LLMs clearly have non-zero correlation, suggesting a degree of consistency."
#| fig.width: 7
#| fig.height: 5

llm_corr_plot
```

```{r, include = FALSE}
linear_model <- lm(score_gemini_score ~ score_chat_gpt_score,
data = scored_llms_df)

summary(linear_model)
```

```{r, include = FALSE}
scored_llms_df |>
group_by(match_binary) |>
tally() |>
ggplot(aes(x = match_binary, y = n, fill = match_binary)) +
geom_bar(stat = "identity") +
scale_fill_brewer(palette = "Accent")
```

### Gemini Summary Findings

The findings above considered score categories (`weak`, `moderate`, and `strong`), while the plots below show risk levels (`low`, `medium`, and `high`). These two metrics are directionally opposed. In other words, `high` risk is associated with `weak` privacy scores.

Of the ten search queries used to collect data here, Gemini summaries were generated for all but the final two (`How to rescue someone who is choking?`" and `What are the best at-home blood pressure monitors to purchase?`).

```{r}
library(stringr)

queries <- c("What does a rash on my arm mean?",
             "How to cope with anxiety?",
             "How to increase chances\n of getting pregnant?",
             "Tips to live with fibromyalgia",
             "What is intermittent fasting\nand how does it work?",
             "What are the benefits of acupuncture?",
             "What are the steps of a colonoscopy?",
             "How to appeal a denied insurance claim?",
             "How to rescue someone who is choking?",
             "What are the best at-home blood pressure\nmonitors to purchase?")

query_df <- cbind.data.frame(queries,
query = LETTERS[1:10])

unscored_breakdown_plt <- 
read_csv("gdrive_whitepaper_data.csv") |>
distinct() |>
mutate(scored = ifelse(score <= 68, "scored", "not scored")) |>
group_by(query, source) |>
count(scored) |>
ungroup() |> 
left_join(query_df) |>
mutate(queries = str_wrap(queries, width = 35)) |>
ggplot(aes(x = scored, y = n, fill = queries)) +
geom_col() +
facet_wrap(queries ~ ., nrow = 3) +
guides(fill = "none") +
xlab("PrivacyLens Scoring Status") +
ylab("Count") +
ggtitle("PrivacyLens Scoring Status by Query of Unique Domains")
```

```{r, include = FALSE}
read_csv("gdrive_whitepaper_data.csv") |>
    mutate(scored = ifelse(score <= 68, "scored", "not scored")) |>
    group_by(source) |>
    count(scored) |> ggplot(aes(x = scored, y = n, fill = source)) + geom_col() + facet_wrap(source ~ .)
```

```{r, include=FALSE}
read_csv("gdrive_whitepaper_data.csv") |>
    mutate(scored = ifelse(score <= 68, "scored", "not scored")) |>
    group_by(scored) |>
    count(scored)
```

```{r, include=FALSE}
read_csv("gdrive_whitepaper_data.csv") |>
select(domain, score) |>
distinct() |>
mutate(scored = ifelse(score <= 68, "scored", "not scored")) |>
group_by(scored) |>
count(scored)
```

Of the 339 unique domains that appeared in the search results accross all 10 queries, PrivacyLens successfully retrieved and scored the privacy policies for 71% of them, illustrating the strong base of domain coverage within PrivacyLens. Due to the same domains appearing in different queries and / or different secctions of the search result within the same query, this coverage encompasses 79% of all the domains in the search result data.

The fraction of unscored websites appears roughly even across all sources (e.g., `gemini_summary`, `organic_result_page_1`, etc.) with the exception of sponsored results. However, because sponsored domains comprise only about 6% of all the domains collected, this discrepancy does not invalidate the following analysis.

As @fig-unscored_domains suggests, PrivacyLens may have better domain coverage for certain types of queries. Only 11% of the domains in the results to the query `How to cope with anxiety?` are unscored compared to 38% of the domains in the results for the query `How to appeal a denied insurance claim?` Because PrivacyLens has an in-built flywheel, domain coverage is poised to grow organically over time.

```{r}
#| label: fig-unscored_domains
#| fig-cap: "While these results suggest some query-to-query variation, PrivacyLens successfully retrieved and scored privacy policies for a majority (71%) of all the unique domains in the search results. Domain coverage is poised to grow organically over time due to PrivacyLens's flywheel."
#| fig.width: 10
#| fig.height: 9

unscored_breakdown_plt
```

```{r, include = FALSE}
# Whitepaper
whitepaper_df <- read_csv("gdrive_whitepaper_data.csv") |>
select(query, page, source, domain, score, top_level_domain) |>
# UPDATE APRIL 16 --> CHECK FOR SCORES OF 0
filter(score <= 68) |>
mutate(privacylens_risk = case_when(
score > 56 ~ "Low",
score >= 40 & score <= 56 ~ "Medium",
score < 40 ~ "High",
TRUE ~ "NA")) |>
mutate(privacylens_risk = factor(privacylens_risk,
levels = c("Low", "Medium", "High"),
ordered = TRUE)) |>
distinct()
```

```{r, include=FALSE}
# Data quality check
whitepaper_df |>
group_by(domain) |>
summarise(num_scores = n_distinct(score)) |>
ungroup() |>
pull(num_scores) |>
max()
```

```{r, include = FALSE}
# Plot 1
domains_vol_by_source <- whitepaper_df |>
distinct() |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
mutate(source = str_wrap(source, width = 10)) |>
ggplot(aes(x = source, fill = source)) +
geom_bar(stat = "count") +
guides(fill = "none") +
xlab("Google Search Origin") +
ylab("Number of Domains") +
ggtitle("Unique Domains by Google Search Source") +
scale_fill_brewer(palette = "Set2")
```

The distribution of domains per source in Google Search is shown in @fig-domain-vol-pg. Despite the fact that only 8 of the 10 search queries resulted in results with Gemini summaries, the number of domains power Gemini summaries exceeds that of any other organic search result page. A few domains from sponsored results are noted here and disregarded in the rest of the analysis since the volume of sponspored domains is quite small and since we are primarily interested in evaluating the privacy of domains used to power Gemini relative to those in organic Google search results.

```{r}
#| label: fig-domain-vol-pg
#| fig-cap: "Unique domain volume by source across all 10 health queries"
#| fig.width: 6
#| fig.height: 4
# Plot 1
domains_vol_by_source
```

When collecting the data, it was observed that many of the same domains in the Gemini summary appear on page one of the organic search results, suggesting that perhaps Google uses popular sites to power Gemini. For several queries, Gemini used at least half of the domains in the page one organic search results for its AI summaries, as shown in @fig-gemini-frequent.

```{r, include = FALSE}
# Using this logic to manually compile gemini_overlap.csv
# Overlap between gemini summary and page 1 domains

# Inclusive data here because we do not require scores to compute overlaps
whitepaper_df_inclusive <- read_csv("gdrive_whitepaper_data.csv") |>
select(query, page, source, domain, score)

pg_one_domains <- numeric()
prop_organic_one_gemini <- numeric()

for (index in c(1:8)) {

QUERY <- LETTERS[index]

gemini_domains <- whitepaper_df_inclusive |>
filter(query == QUERY) |>
filter(!query %in% c("I", "J")) |>
filter(source == "gemini_summary") |>
pull(domain) |>
unique()

organic_pg_one_domains <- whitepaper_df_inclusive |>
filter(query == QUERY) |>
filter(!query %in% c("I", "J")) |>
filter(source == "organic_result_page_1") |>
pull(domain) |>
unique()

pg_one_domains[index] <- length(organic_pg_one_domains)

# What proportion of page 1 (organic) results are used in the gemini summary?
prop_organic_one_gemini[index] <- length(intersect(gemini_domains, organic_pg_one_domains))/length(organic_pg_one_domains)

}

gemini_overlap <- cbind.data.frame(
query_df[1:8,],
pg_one_domains,
prop_organic_one_gemini) |>
mutate(queries = str_wrap(queries, width = 20))
```

```{r, include = FALSE}
gemini_overlap |>
ggplot(aes(x = fct_reorder(queries,
prop_organic_one_gemini,
.desc = TRUE), y = prop_organic_one_gemini)) +
geom_bar(stat = "identity", fill = "#848FA5") +
coord_flip() +
xlab("Quesiton") +
ylab("Proportion of Page-One Organic Domains Used in Gemini Summary") +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Gemini Summaries Frequently Enlist Domains \nfrom First-Page Organic Search Results")
```

```{r, include = FALSE}
# Plot 2
gemini_overlap_plot <- gemini_overlap |>
filter(!query %in% c("I", "J")) |>
ggplot(aes(x = pg_one_domains, y = prop_organic_one_gemini)) +
geom_point(size = 3, color = "#f2542d")  +
ggrepel::geom_text_repel(aes(label = queries),
max.overlaps = Inf,
size = 4,
box.padding = 0.5,
point.padding = 0.3,
segment.color = "#f2542d",
color = "#f2542d") +
ggtitle("Gemini Summaries Frequently Use Domains \nfrom First-Page Organic Search Results") +
xlab("Number of Unique Domains in First-Page Organic Search Results *") +
ylab("Proportion of Page-One\nOrganic Domains Used in Gemini Summary") +
scale_y_continuous(labels = scales::percent_format()) +
labs(caption = "* See Whitepaper methodology for details")

gemini_overlap_plot
```

```{r}
#| label: fig-gemini-frequent
#| fig-cap: "Domains occuring in the first page of organic search results are frequently used in Google Gemini summaries. For instance, 7 of the 11 unique domains occuring in the organic results on page 1 for the query 'Tips to live with fibromyalgia?' were used to power Gemini's summary for that query."
#| fig.width: 9
#| fig.height: 6


# Plot 2
gemini_overlap_plot
```

To better understand the overlap between domains used to power Gemini and those appearing on page one organic search results, we can classify each (non-sponsored) domain on the first page of Google search results as one that powers Gemini, one served in the organic search reuslts, or one that appears in both groups. The Venn diagram below illustrates classification.

![Domains on the first page can be classified into those appearing in Gemini only, those in Page 1 Organic only, or those domains in both, as the diagram above illustrates.](venn_diagram.png){width="40%"}

The plot below, @fig-score-by-origin, shows how the risk categories of the domains in each of these groups is distributed. Note that plot treats each of these three three groups as mutually exclusive. (To be clear, if a domain appears in both Gemini's summary and in page one organic search results, the domain belongs to the "Both" category only and not to all three categories.) Doing so allows us to compare these three groups against one another free of the interference of double-counting the same domain in two separate groups.

```{r, include = FALSE}
# distribution of gemini organic page one results of those domains
# in gemini vs. not in gemini

gemini_domains <- whitepaper_df |>
filter(!query %in% c("I", "J")) |>
select(query, source, domain, privacylens_risk) |>
distinct() |>
filter(source == "gemini_summary") |>
mutate(in_gemini = 1) |>
distinct()

pg_one_domains <- whitepaper_df |>
filter(!query %in% c("I", "J")) |>
select(query, source, domain, privacylens_risk) |>
distinct() |>
filter(source == "organic_result_page_1") |>
mutate(in_page_one = 1) |>
distinct()

domain_origins <- pg_one_domains |>
full_join(gemini_domains, by = c("query", "domain",
"privacylens_risk")) |>
mutate(origin = case_when(
in_page_one == 1 & in_gemini == 1 ~ "Both",
in_page_one == 1 & is.na(in_gemini) ~ "Page One Organic Only",
TRUE ~ "Gemini Only")) |>
distinct()
```

```{r, include = FALSE}
# Plot 3
# venn_diagram_plot <-
# domain_origins |>
# mutate(privacylens_risk = factor(privacylens_risk,
# levels = c("Low", "Medium", "High"),
# ordered = TRUE)) |>
# ggplot(aes(x = privacylens_risk, fill = origin)) +
# geom_bar(stat = "count", alpha = 0.7,
# position = "dodge") +
# facet_wrap(origin ~ ., scales = "free_y", ncol = 1) +
# scale_fill_brewer(palette = "Dark2",
# name = "Domain Origin") +
# guides(fill = "none")

score_by_origin <- domain_origins |>
mutate(privacylens_risk = factor(privacylens_risk,
levels = c("Low", "Medium", "High"),
ordered = TRUE)) |>
group_by(origin) |>
tally() |>
rename(total_n = n) |>
left_join(
domain_origins |>
mutate(privacylens_risk = factor(privacylens_risk,
levels = c("Low", "Medium", "High"),
ordered = TRUE)) |>
group_by(origin, privacylens_risk) |>
tally()) |>
mutate(prop = n / total_n) |>
mutate(origin = factor(origin, levels = c("Gemini Only", "Both",
"Page One Organic Only"),
ordered = TRUE)) |>
ggplot(aes(x = privacylens_risk, y = prop, fill = origin)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.9) +
scale_fill_manual(values = c("#b8b5d1", "#75abbf", "#99d2d9"),
name = "Domain Origin") +
# scale_fill_brewer(palette = "Dark2",
# name = "Domain Origin") +
xlab("PrivacyLens Risk") +
ylab("Proportion of Origin Domains\nin Risk Category") +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Distribution of Risk Categories by Origin:\nFirst Page of Search Results")

score_by_origin
```

```{r}
#| label: fig-score-by-origin
#| fig-cap: "As the plot shows, a majority (over 60%) of domains appearing in both page one organic search results and used to power Gemini summaries are classified as low-risk. None of the page one organic search result domains (that were not also used to power Gemini summaries) were deemed to be high risk, suggesting that perhaps page 1 organic search results may perform favorably privacy-wise relative to domains powering Gemini. Data from the two queries that did not result in any Gemini summaries are excluded here. "
#| fig.width: 8
#| fig.height: 4


# Plot 3
score_by_origin
```

The distribution of risk categories by search section \\ page are shown in @fig-score-by-pg-seq. In no sections of the search results do high-risk domains comprise the majority, which is favorable.

```{r, include = FALSE}
# Plot 4
scores_by_pg_plot <- whitepaper_df |>
distinct() |>
filter(!source %in% "sponsored_result") |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
# filter(source == "Page 2 Organic") |>
group_by(source) |>
count(privacylens_risk) |>
mutate(prop = n / sum(n)) |>
filter(source != "Sponsored") |>
ggplot(aes(x = privacylens_risk, y = prop, fill = source)) +
# ggplot(aes(x = privacylens_risk, fill = source)) +
geom_col() +
facet_wrap(source ~ .,) +
xlab("PrivacyLens Risk") +
ylab("Count") +
scale_fill_viridis_d(name = "Domain Source") +
theme_linedraw() +
ggtitle("Distribution of PrivacyLens Risk Score by Website Origin") +
guides(fill = "none") +
scale_y_continuous(labels = scales::percent_format(),
limits = c(0, 0.55))
```

```{r}
#| label: fig-score-by-pg-seq
#| fig-cap: "The proportion of domains belgonging to each risk category by Google search section / page."
#| fig.width: 7
#| fig.height: 5


# Plot 4
scores_by_pg_plot
```

```{r}
scoring_trend_df <- whitepaper_df |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
group_by(source) |>
summarise(median_score = median(score),
num_domains = n_distinct(domain)) |>
ungroup() |>
mutate(group_num = as.numeric(factor(source))) |>
filter(source != "Sponsored") |>
mutate(source = str_wrap(source, width = 10))

scoring_model <- lm(median_score ~ group_num, data = scoring_trend_df)

trend_plot <- whitepaper_df |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
mutate(source = str_wrap(source, width = 10)) |>
filter(source != "Sponsored") |>
ggplot(aes(x = source, y = score, fill = source)) +
geom_boxplot() +
guides(fill = "none") +
xlab("Search Result Source") +
ylab("Score") +
ggtitle("PrivacyLens Score by Source")
```

```{r, include=FALSE}
# Appendix Slides

whitepaper_df |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
mutate(source = str_wrap(source, width = 10)) |>
filter(source != "Sponsored") |>
ggplot(aes(x = source, y = score, col = source)) +
geom_boxplot() +
guides(fill = "none") +
xlab("Search Result Source") +
ylab("Score") +
ggtitle("PrivacyLens Score by Source") +
guides(color = "none") +
geom_point(data = scoring_trend_df, aes(x = source, y = median_score), size = 3) +
geom_smooth(data = scoring_trend_df, aes(x = group_num, y = median_score),
method = "lm", se = TRUE, color = "black", fill = "darkblue")

whitepaper_df |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
filter(source != "Sponsored") |>
filter(source %in% c("Gemini Summary", "Page 1 Organic", "Page 2 Organic")) |> mutate(source = str_wrap(source, width = 10)) |>
ggplot(aes(x = source, y = score, col = source)) +
geom_boxplot() +
guides(fill = "none") +
xlab("Search Result Source") +
ylab("Score") +
ggtitle("PrivacyLens Score by Source") +
guides(color = "none") +
geom_point(data = scoring_trend_df |>
filter(source %in% c("Gemini\nSummary", "Page 1\nOrganic", "Page 2\nOrganic")),
aes(x = source, y = median_score), size = 3) +
geom_smooth(data = scoring_trend_df |>
filter(source %in% c("Gemini\nSummary", "Page 1\nOrganic", "Page 2\nOrganic")),
aes(x = group_num, y = median_score),
method = "lm", se = TRUE, color = "black", fill = "darkblue")
```

As @fig-trend-plot suggests, privacy scores are roughly consistent from one section of the search results to the next. There may be some evidence that privacy scores decrease as one navigates away from the Gemini summary / page one, but few users likely reach pages 4 and 5.

```{r}
#| label: fig-trend-plot
#| fig-cap: "This plot shows the distribution of PrivacyLens scores by source. Though it is subtle, the medians in each boxplot suggests that privacy scores tend to decrease as we navigate further away from Gemini / Page 1 onto Pages 4 and 5. However, given the fact that most Google users likely do not reach pages 3 and beyond, there also seems to be evidence to suggest that privacy outcomes improve as the user navigates away from Gemini onto organic search results on pages 1 and 2. Generally speaking, the distribution of scores do not appear to vary markedly from one page to another."
#| fig.width: 7
#| fig.height: 5

trend_plot
```

<!-- Individual, page-by-page comparisons are easier to discern in the plot below. The high risk category generally decreases, while the low category generally increases. -->

```{r, skip = TRUE}
# imgs <- list.files(path = "score-gif", pattern = "*.png", full.names = TRUE) |>
# lapply(image_read) |>    # Read images
# image_join() |>          # Combine into an animation
# image_animate(fps = 1)  # Adjust frames per second (fps)

# Save the GIF
# image_write(imgs, "score.gif")
```

<!-- ![The proprotion of sites from low-risk domains generally tends to increase as we navigate further from the Gemini summary.](score.gif){width="70%"} -->

As shown in @fig-risk-incr, as the user navigates away from the Gemini summary, the proportion of high-risk domains shown to her as she navigates successive pages generally tends to increase, as the regression line indicates. The size of the point is proportional to the number of domains shown on the respective page across all search queries.

```{r}
#| label: fig-risk-incr
#| fig-cap: "The further away from the Gemini summary the user navigates, the proportion of high-risk domains shown to the user generally increases. Sponsored domains are excluded from this plot."
#| fig.width: 8
#| fig.height: 5

whitepaper_df |>
distinct() |>
filter(!source %in% "sponsored") |>
mutate(source = recode(source,
"gemini_summary" = "Gemini Summary",
"organic_result_page_1" = "Page 1 Organic",
"organic_result_page_2" = "Page 2 Organic",
"organic_result_page_3" = "Page 3 Organic",
"organic_result_page_4" = "Page 4 Organic",
"organic_result_page_5" = "Page 5 Organic",
"sponsored" = "Sponsored")) |>
group_by(source) |>
count(privacylens_risk) |>
group_by(source) |>
mutate(total_n = sum(n)) |>
ungroup() |>
mutate(prop = n / total_n) |>
group_by(source) |>
filter(privacylens_risk == "High") |>
ungroup() |>
mutate(x_marker = row_number()) |>
# filter(source %in% c("Gemini Summary", "Page 1 Organic", "Page 2 Organic")) |> 
ggplot(aes(x = x_marker, y = prop, size = total_n)) +
geom_point(col = "#c84c09") +
geom_smooth(method = "lm", se = TRUE, size = 1,
color = "#420217", show.legend = FALSE, fill = "lightblue") +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Proportion of High-Privacy-Risk Domains by Source") +
scale_size(name = "Total Domains") +
labs(x = "Source", y = "Proportion of High-Risk Privacy Domains") +
theme(axis.title.x = element_text(size = 14), axis.title.y = element_text(size = 14)) +
scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6),
labels = c("Gemini\nSummary", "Page 1\nOrganic", "Page 2\nOrganic", "Page 3\nOrganic", "Page 4\nOrganic", "Page 5\nOrganic")) +
theme_linedraw()
```

However, when analyzing results query-by-query, as shown in @fig-query-breakdown, this aggregate trend does not *always* hold, though eight of the ten plots below have a negative slope. Still, to the extent that a user may accumulate multiple queries over time -- as is the case for most users -- the aggregate trend may not be spurious.

```{r}
#| label: fig-query-breakdown
#| fig-cap: "When analyzing the above trend query-by-query, this aggregate finding appears to hold in the vast majority of queries, though it is slightly less pronounced. Still, since most users conduct multiple searches, the above trend may not be spurious."
#| fig.width: 12
#| fig.height: 10


xval_df <- cbind.data.frame(x_val = c(1:6),
source = c("Gemini Summary", "Page 1 Organic", "Page 2 Organic", "Page 3 Organic", "Page 4 Organic", "Page 5 Organic"))


whitepaper_df |>
  filter(!source %in% "sponsored_result")|>
  mutate(source = recode(source,
                         "gemini_summary" = "Gemini Summary",
                         "organic_result_page_1" = "Page 1 Organic",
                         "organic_result_page_2" = "Page 2 Organic",
                         "organic_result_page_3" = "Page 3 Organic",
                         "organic_result_page_4" = "Page 4 Organic",
                         "organic_result_page_5" = "Page 5 Organic",
                         "sponsored_result" = "Sponsored")) |>
  group_by(query, source) |>
  count(privacylens_risk) |>
  group_by(query, source) |>
  mutate(total_n = sum(n)) |>
  ungroup() |>
  mutate(prop = n / total_n) |>
  group_by(query, source) |>
  filter(privacylens_risk == "High") |>
  ungroup() |>
  group_by(query) |>
  left_join(xval_df) |>
  ungroup() |>
  mutate(query = recode(query,
                         "A" = queries[1],
                         "B" = queries[2],
                         "C" = queries[3],
                         "D" = queries[4],
                         "E" = queries[5],
                         "F" = queries[6],
                         "G" = queries[7],
                         "H" = queries[8],
                         "I" = queries[9],
                         "J" = queries[10])) |>
  ggplot(aes(x = x_val, y = prop, size = total_n, col = query)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, size = 1,
              color = "gray", show.legend = FALSE) +
  facet_wrap(query~.) +
  scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6),
                     labels = c("Gem.\nSum.", "Pg.\n1", "Pg.\n2", "Pg.\n3", "Pg.\n4", "Pg.\n5")) +
  guides(color = "none") +
  scale_size(name = "Total Domains") +
  ggtitle("Proportion of High-Risk Privacy Domains by Query and Source") +
  scale_y_continuous(labels = scales::percent_format()) +
  xlab("") +
  ylab("Proportion of High-Risk Privacy Domains") +
theme_minimal()

```

```{r, include = FALSE}
whitepaper_df |>
group_by(source) |>
tally() |>
rename(total_n = n) |>
right_join(
whitepaper_df |>
group_by(source, privacylens_risk) |>
tally())
```

## Discussion

The analysis above illustrates that some evidence of score consistency exists among different large language models, providing some assurance regarding the scores output by PrivacyLens. That said, while there is substantial room for greater consistency to be demonstrated, even perfect consistency (i.e., $\rho = 1$) is not a foolproof guarantee of accuracy. It is possible, for instance, that multiple LLMs generating these risk scores are similarly biased, leading to an outcome in which they demonstrate strong consistency with one another -- in other words, they appear precise -- but lack accuracy. A full validation study enlisting experts to score policies is best, and these gold standard evaluations can be used as a souce of truth against which the LLM risk scores can be measured.

Additionally, Gemini appears to generate summaries from domains with comparable privacy scores compared to domains that appear in Google's organic search results (pages 1 through 5). Given the fact that the number of domains used to power Gemini is larger than the number of domains on any other page and that domains powering Gemini receive more clicks, as Google has published, there seems to be evidence that Google could improve the privacy risks to which it exposes its users by taking more proactive privacy-related metrics into account when ranking websites. There appears to be strong evidence that many page-one organic search result domains are used in Gemini summaries.

Future improvements to this work include addressing some of the methodological challenges (including an evaluation of these results using domain counts weighted by their frequency in search result pages), sampling a more representative group of domains, inspecting patterns in which domains are scored favorably by which LLMs, and conducting a proper validation study in which gold standard scores from experts can be compared against LLM outputs. A closer inspection at patterns among top-level domains, as shown in @fig-tld-risks below, is an avenue for future research, as is assessing how these results adapt when accounting for the 29% of domains PrivacyLens failed to auto-score.

```{r}
#| label: fig-tld-risks
#| fig-cap: "The distribution of risk score by top-level domain. Interestingly, .edu and .gov top-level-domains appear to have the greatest room for improvement, as their respective proportions of high-risk domains exceeds that of all the others."
#| fig.width: 7
#| fig.height: 5

whitepaper_df |>
filter(top_level_domain %in% c("com", "org", "gov", "edu", "au", "uk")) |>
group_by(top_level_domain, privacylens_risk) |>
count(privacylens_risk) |>
ggplot(aes(x = privacylens_risk, y = n, fill = top_level_domain)) +
geom_col() +
facet_wrap(top_level_domain ~ ., scales = "free_y") +
scale_fill_brewer(palette = "Paired") +
xlab("PrivacyLens Risk") +
ylab("Proportion of Domains") +
labs(
title = "Distribution of Privacy Risk by Top-Level Domain",
subtitle = "Top-Level Domains with ≥ 10 domains in data") +
guides(fill = "none")
```

Studying how specific government websites, the sites of top American hospitals, and popular health sites fare with respect to privacy risk is all fodder for future work. Understanding the extent to which these results generalize to other countries, languages, and sectors (e.g., beyond healthcare) is a natural extension of this initial analysis.

## Conclusion

The raw privacy scores output by PrivacyLens via ChatGPT share a 33% correlation with the scores output by Google Gemini for the same domains using the same set of scoring instructions. Evidence of LLM consistency may be a sign of precision, but because LLMs might be similarly biased, the observed consistency is not foolproof evidence of accuracy. While the evidence here disproves the idea that the scores output by PrivacyLens are random, a proper validation study with expert-generated gold standards is the best way to assess accuracy.

Furthermore, based on the data collected, there appears to grounds upon which to claim that domains used to power Google Gemini are roughly equal in privacy protections relative to domains in Google's organic search results. For the Google queries used here, a substantial proportion of page one organic search result domains are used in Gemini summaries. As one navigates further away from the Gemini summary, results shown to the user generally appear to a higher proportion of high-risk domains, though this trend is not strictly monotonic.

## References
